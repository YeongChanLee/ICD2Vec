{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText as fText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('./wiki.en.vec')\n",
    "model_words = list(model.wv.vocab)     #Loading the model words\n",
    "model_bin = fText.load_fasttext_format(\"./wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the model using gensim. 'model_words' contains the word in which are there in the dict of pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')      #loading the stopwords\n",
    "stemmer = SnowballStemmer(\"english\")         #loading the stemmer\n",
    "\n",
    "delimiter = [',', ':','!', '@','&','$','.','/',']','[']     #defining a set of delimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_encoding = dict()\n",
    "flag = 0\n",
    "\n",
    "with open(\"./ukb_coding19.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "    \tif (flag==0):\n",
    "    \t\tflag = 1\n",
    "    \t\tcontinue\n",
    "    \ta = row[1].split(' ',1)[1]\n",
    "    \tdata_encoding[row[0]]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_syn = dict()\n",
    "cli_info = dict()\n",
    "\n",
    "with open(\"./icd_info4.csv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\",\", quotechar='\"')\n",
    "    for row in rd:\n",
    "    \trow[0] = row[0].split('-')[0]\n",
    "    \ttry:\n",
    "    \t\trow[0] = row[0].split('.')[0] + row[0].split('.')[1]\n",
    "    \texcept:\n",
    "    \t\trow[0] = row[0].split('.')[0]\n",
    "    \tcli_info[row[0]] = row[3]\n",
    "    \tapp_syn[row[0]] = row[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the approximate synonyms, clincal information and ICD code description in three different dict file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def process(value):       #This method gives a single vector corresponding to the value passed.\n",
    "\tword_count = 0\n",
    "\tvalue_vec = np.array([0] * 300)\n",
    "\tfor word in word_tokenize(value):\n",
    "\t\tword = word.lower()\n",
    "\t\tif(word in model_words and word not in stop_words and word not in delimiter):\n",
    "\t\t\tvalue_vec = value_vec + model.wv[word]\n",
    "\t\t\tword_count = word_count + 1\n",
    "\t\telif(stemmer.stem(word) in model_words and stemmer.stem(word) not in stop_words and stemmer.stem(word) not in delimiter):\n",
    "\t\t\tvalue_vec = value_vec + model.wv[stemmer.stem(word)]\n",
    "\t\t\tword_count = word_count + 1\n",
    "\t\telif(word not in stop_words and word not in delimiter):\n",
    "\t\t\tvalue_vec = value_vec + model_bin.wv[word]\n",
    "\t\t\tword_count = word_count + 1\n",
    "\n",
    "\tif(word_count>0):\n",
    "\t\tvalue_vec = value_vec/word_count\n",
    "\n",
    "\treturn value_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-23f7683fc3c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0micd_code_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micd_code_desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0micd_code_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micd_code_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-83fe3b610c0a>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                         \u001b[0mvalue_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_vec\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_words' is not defined"
     ]
    }
   ],
   "source": [
    "icd_code_vec = dict()     #for storing vector corresponding to each ICD code\n",
    "for key, value in data_encoding.items():\n",
    "    icd_code_desc = value\n",
    "    try:\n",
    "        if(len(app_syn[key])>0):\n",
    "            icd_code_desc = icd_code_desc + \" \" + app_syn[key]\n",
    "        if(len(cli_info[key])>0):\n",
    "            icd_code_desc = icd_code_desc + \" \" + cli_info[key]\n",
    "    except:\n",
    "        icd_code_desc = icd_code_desc\n",
    "    icd_code_vec[key] = process(icd_code_desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
